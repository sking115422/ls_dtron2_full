{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detectron Object Detection - Test, Train, Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spenc/miniconda3/envs/dtron2_venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1+cu111 True\n"
     ]
    }
   ],
   "source": [
    "# install dependencies: (use cu101 because colab has CUDA 10.1)\n",
    "# opencv is pre-installed on colab\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "import gc\n",
    "#del variables\n",
    "gc.collect()\n",
    "# Gong added this:\n",
    "torch.cuda.empty_cache()\n",
    "#torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "\n",
    "# You may need to restart your runtime prior to this, to let your installation take effect\n",
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.data.catalog import DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "\n",
    "#from .detectron2.tools.train_net import Trainer\n",
    "#from detectron2.engine import DefaultTrainer\n",
    "# select from modelzoo here: https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md#coco-object-detection-baselines\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "#from detectron2.evaluation.coco_evaluation import COCOEvaluator\n",
    "import os\n",
    "\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "import glob\n",
    "\n",
    "import random\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "import numpy as np \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Register Custom D2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_img_refs (in_dir):\n",
    "\n",
    "    f = open(in_dir + \"/result.json\")\n",
    "    coco_json = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    img_list = coco_json[\"images\"]\n",
    "\n",
    "    for i in range(0, len(img_list)):\n",
    "        img_name = img_list[i][\"file_name\"].split(\"/\")[-1]\n",
    "        new_path = \"./\" + img_name\n",
    "        coco_json[\"images\"][i][\"file_name\"] = new_path\n",
    "\n",
    "    j_out = json.dumps(coco_json, indent=4)\n",
    "\n",
    "    os.remove(in_dir + \"/result.json\")\n",
    "\n",
    "    f = open(in_dir + \"/result.json\", \"w\")\n",
    "    f.write(j_out)\n",
    "    f.close()\n",
    "    \n",
    "    print (\"updating image references\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_train_test_split (in_dir):\n",
    "    \n",
    "    fn = in_dir.split(\"/\")[-1]\n",
    "    \n",
    "    if fn == None:\n",
    "        fn = in_dir.split(\"/\")[-2]\n",
    "  \n",
    "    out_dir = os.getcwd() + \"/\" + fn + \"_split\"\n",
    "    \n",
    "    if not os.path.exists(out_dir):\n",
    "\n",
    "        os.mkdir(out_dir)\n",
    "\n",
    "        train_dir = out_dir + \"/train\"\n",
    "        os.mkdir(out_dir + \"/train\")\n",
    "        train_img_dir = train_dir + \"/images\"\n",
    "        os.mkdir(train_img_dir)\n",
    "\n",
    "        test_dir = out_dir + \"/test\"\n",
    "        os.mkdir(out_dir + \"/test\")\n",
    "        test_img_dir = test_dir + \"/images\"\n",
    "        os.mkdir(test_img_dir)\n",
    "\n",
    "        train_split = 0.8\n",
    "\n",
    "        f = open (in_dir + \"/result.json\")\n",
    "\n",
    "        coco_json = json.load(f)\n",
    "\n",
    "        num_img = len(coco_json[\"images\"])\n",
    "\n",
    "        img_list = coco_json[\"images\"]\n",
    "        cat_list = coco_json[\"categories\"]\n",
    "        ann_list = coco_json[\"annotations\"]\n",
    "\n",
    "        train_num = math.floor(num_img * train_split)\n",
    "\n",
    "        train_img_list = img_list[0:train_num]\n",
    "        test_img_list = img_list[train_num:]\n",
    "\n",
    "        for each in train_img_list:\n",
    "            img_name = each[\"file_name\"].split(\"/\")[-1]\n",
    "            shutil.copy(in_dir + \"/images/\" + img_name, train_img_dir + \"/\" + img_name)\n",
    "\n",
    "        for each in test_img_list:\n",
    "            img_name = each[\"file_name\"].split(\"/\")[-1]\n",
    "            shutil.copy(in_dir + \"/images/\" + img_name, test_img_dir + \"/\" + img_name)\n",
    "\n",
    "        co_val = train_img_list[-1][\"id\"]\n",
    "\n",
    "        train_ann_list = [] \n",
    "        test_ann_list = []\n",
    "\n",
    "        for each in ann_list:\n",
    "\n",
    "            if each[\"image_id\"] <= co_val:\n",
    "                train_ann_list.append(each)\n",
    "            else:\n",
    "                test_ann_list.append(each)\n",
    "\n",
    "        train_json = {\n",
    "            \"images\" : train_img_list,\n",
    "            \"categories\": cat_list,\n",
    "            \"annotations\":train_ann_list\n",
    "        }\n",
    "\n",
    "        test_json = {\n",
    "            \"images\" : test_img_list,\n",
    "            \"categories\": cat_list,\n",
    "            \"annotations\":test_ann_list\n",
    "        }\n",
    "\n",
    "        train_j_out = json.dumps(train_json, indent=4)\n",
    "        test_j_out = json.dumps(test_json, indent=4)\n",
    "\n",
    "        with open(train_dir + \"/result.json\", \"w\") as outfile:\n",
    "            outfile.write(train_j_out)\n",
    "        with open(test_dir + \"/result.json\", \"w\") as outfile:\n",
    "            outfile.write(test_j_out)\n",
    "            \n",
    "        print (\"creating \" + str(train_split) + \" train test split to path: \" + out_dir)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print(\"file \" + out_dir + \" already exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating image references\n",
      "file /home/spenc/research/ls_dron2/ls_detectron2_objdet/model/coco_prelab_743_n_bal_split already exists!\n",
      "training set coco instance registered\n",
      "test set coco instance registered\n"
     ]
    }
   ],
   "source": [
    "# Setting paths\n",
    "\n",
    "coco_input_base_dir =  \"./../coco_files_bal/\"        \n",
    "input_fn = \"coco_prelab_743_n_bal\"\n",
    "\n",
    "update_img_refs(coco_input_base_dir + input_fn)\n",
    "coco_train_test_split(coco_input_base_dir + input_fn) \n",
    "    \n",
    "register_coco_instances(\"my_dataset_train\", {}, \"./\" + input_fn + \"_split/train/result.json\",\"./\" + input_fn + \"_split/train/images\")\n",
    "print(\"training set coco instance registered\")\n",
    "register_coco_instances(\"my_dataset_test\", {}, \"./\" + input_fn + \"_split/test/result.json\",\"./\" + input_fn + \"_split/test/images\")\n",
    "print(\"test set coco instance registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Custom D2 Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are importing our own Trainer Module here to use the COCO validation evaluation during training. Otherwise no validation eval occurs.\n",
    "\n",
    "class CocoTrainer(DefaultTrainer):\n",
    "\n",
    "  @classmethod\n",
    "  def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "\n",
    "    if output_folder is None:\n",
    "        os.makedirs(\"coco_eval\", exist_ok=True)\n",
    "        output_folder = \"coco_eval\"\n",
    "\n",
    "    return COCOEvaluator(dataset_name, cfg, False, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting model configs\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
    "cfg.DATASETS.TEST = (\"my_dataset_test\",)\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "cfg.SOLVER.BASE_LR = 0.001\n",
    "\n",
    "### Testing\n",
    "# cfg.SOLVER.MAX_ITER = 20\n",
    "\n",
    "### OG\n",
    "cfg.SOLVER.WARMUP_ITERS = 1000\n",
    "cfg.SOLVER.MAX_ITER = 3000 #adjust up if val mAP is still rising, adjust down if overfit\n",
    "cfg.SOLVER.STEPS = (1000, 1300, 1800)\n",
    "cfg.SOLVER.GAMMA = 0.05\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 32 + 1 #your number of classes + 1\n",
    "\n",
    "### OG\n",
    "cfg.TEST.EVAL_PERIOD = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "print (\"model training started...\")\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = CocoTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "\n",
    "print (\"model testing started...\")\n",
    "\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.85\n",
    "predictor = DefaultPredictor(cfg)\n",
    "evaluator = COCOEvaluator(\"my_dataset_test\", cfg, False, output_dir=\"./output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"my_dataset_test\")\n",
    "inference_on_dataset(trainer.model, val_loader, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with D2 Saved Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soi(str1, start_char, end_char):\n",
    "    str1 = str(str1)\n",
    "    offst = len(start_char)\n",
    "    ind1 = str1.find(start_char)\n",
    "    ind2 = str1.find(end_char)\n",
    "    s_str = str1[ind1+offst:ind2]\n",
    "    return s_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataDict (fn, outputs):\n",
    "\timg_shape = list(outputs[\"instances\"].image_size)\n",
    "\timg_h = int(img_shape[0])\n",
    "\timg_w = int(img_shape[1])\n",
    "\n",
    "\tclass_list = get_soi(outputs[\"instances\"].pred_classes, \"[\", \"]\").split(\",\")\n",
    "\tclass_list_new = []\n",
    "\tfor each in class_list:\n",
    "\t\tclass_list_new.append(int(each.strip()))\n",
    "\n",
    "\tbbox_list = get_soi(outputs[\"instances\"].pred_boxes, \"[[\", \"]]\").split(\"]\")\n",
    "\tbbox_list_new = []\n",
    "\tfor each in bbox_list:\n",
    "\t\tbbox = re.sub(\"['[,\\n]\", \"\", each).split(\" \")\n",
    "\t\tbbox_new = []\n",
    "\t\tfor item in bbox:\n",
    "\t\t\tif item != \"\":\n",
    "\t\t\t\tbbox_new.append(float(item))\n",
    "\t\tbbox_list_new.append(bbox_new)\n",
    "\n",
    "\tann_list = []\n",
    "\tfor i in range(0, len(class_list)):\n",
    "\t\t# og was \"bbox_mode\": \"<BoxMode.XYWH_ABS: 1>\"\n",
    "\t\tann_list.append({\"iscrowd\": 0, \"bbox\": bbox_list_new[i], \"category_id\": class_list_new[i], \"bbox_mode\": 0})\n",
    "\t\n",
    "\tdata_dict = {\n",
    "\t\t\"file_name\": fn,\n",
    "\t\t\"height\": img_h,\n",
    "\t\t\"width\": img_w, \n",
    "\t\t\"annotations\": ann_list\n",
    "\t}\n",
    " \n",
    "\treturn data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### File paths\n",
    "\n",
    "img_out_dir = \"./img_out/\"\n",
    "# img_in_dir = \"./test_inf_imgs\"\n",
    "img_in_dir = \"./\" + input_fn + \"_split/test/images/\"\n",
    "results_dir = \"./results\"\n",
    "\n",
    "if not os.path.exists(img_out_dir):\n",
    "    os.makedirs(img_out_dir)\n",
    "    \n",
    "if not os.path.exists(img_in_dir):\n",
    "    os.makedirs(img_in_dir)\n",
    "    \n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model inference started...\n",
      "\u001b[32m[09/11 12:59:39 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./output/model_final.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'proposal_generator.rpn_head.conv.weight' to the model due to incompatible shapes: (256, 256, 3, 3) in the checkpoint but (1024, 1024, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'proposal_generator.rpn_head.conv.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (1024,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'proposal_generator.rpn_head.objectness_logits.weight' to the model due to incompatible shapes: (3, 256, 1, 1) in the checkpoint but (15, 1024, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'proposal_generator.rpn_head.objectness_logits.bias' to the model due to incompatible shapes: (3,) in the checkpoint but (15,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'proposal_generator.rpn_head.anchor_deltas.weight' to the model due to incompatible shapes: (12, 256, 1, 1) in the checkpoint but (60, 1024, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'proposal_generator.rpn_head.anchor_deltas.bias' to the model due to incompatible shapes: (12,) in the checkpoint but (60,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (34, 1024) in the checkpoint but (81, 2048) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (34,) in the checkpoint but (81,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (132, 1024) in the checkpoint but (320, 2048) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (132,) in the checkpoint but (320,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.res2.0.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res2.0.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.res2.0.conv2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res2.0.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.res2.0.conv3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res2.0.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.res2.0.shortcut.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res2.0.shortcut.weight\u001b[0m\n",
      "\u001b[34mbackbone.res2.1.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res2.1.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.res2.1.conv2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res2.1.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.res2.1.conv3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res2.1.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.res2.2.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res2.2.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.res2.2.conv2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res2.2.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.res2.2.conv3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res2.2.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.res3.0.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res3.0.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.res3.0.conv2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res3.0.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.res3.0.conv3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res3.0.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.res3.0.shortcut.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res3.0.shortcut.weight\u001b[0m\n",
      "\u001b[34mbackbone.res3.1.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res3.1.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.res3.1.conv2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res3.1.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.res3.1.conv3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res3.1.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.res3.2.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res3.2.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.res3.2.conv2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res3.2.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.res3.2.conv3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res3.2.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.res3.3.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res3.3.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.res3.3.conv2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res3.3.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.res3.3.conv3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res3.3.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.res4.0.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res4.0.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.res4.0.conv2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res4.0.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.res4.0.conv3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res4.0.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.res4.0.shortcut.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res4.0.shortcut.weight\u001b[0m\n",
      "\u001b[34mbackbone.res4.1.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res4.1.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.res4.1.conv2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res4.1.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.res4.1.conv3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res4.1.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.res4.2.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res4.2.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.res4.2.conv2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res4.2.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.res4.2.conv3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res4.2.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.res4.3.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res4.3.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.res4.3.conv2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res4.3.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.res4.3.conv3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res4.3.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.res4.4.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res4.4.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.res4.4.conv2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res4.4.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.res4.4.conv3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res4.4.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.res4.5.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res4.5.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.res4.5.conv2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res4.5.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.res4.5.conv3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.res4.5.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.stem.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.stem.conv1.weight\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.conv.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.res5.0.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.res5.0.conv1.weight\u001b[0m\n",
      "\u001b[34mroi_heads.res5.0.conv2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.res5.0.conv2.weight\u001b[0m\n",
      "\u001b[34mroi_heads.res5.0.conv3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.res5.0.conv3.weight\u001b[0m\n",
      "\u001b[34mroi_heads.res5.0.shortcut.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.res5.0.shortcut.weight\u001b[0m\n",
      "\u001b[34mroi_heads.res5.1.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.res5.1.conv1.weight\u001b[0m\n",
      "\u001b[34mroi_heads.res5.1.conv2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.res5.1.conv2.weight\u001b[0m\n",
      "\u001b[34mroi_heads.res5.1.conv3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.res5.1.conv3.weight\u001b[0m\n",
      "\u001b[34mroi_heads.res5.2.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.res5.2.conv1.weight\u001b[0m\n",
      "\u001b[34mroi_heads.res5.2.conv2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.res5.2.conv2.weight\u001b[0m\n",
      "\u001b[34mroi_heads.res5.2.conv3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.res5.2.conv3.weight\u001b[0m\n",
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mbackbone.fpn_lateral2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.fpn_output2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.fpn_lateral3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.fpn_output3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.fpn_lateral4.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.fpn_output4.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.fpn_lateral5.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.fpn_output5.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.stem.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.stem.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.0.shortcut.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.0.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.0.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.0.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.1.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.1.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.1.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.2.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.2.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.2.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.0.shortcut.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.0.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.0.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.0.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.1.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.1.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.1.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.2.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.2.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.2.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.3.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.3.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.3.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.3.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.3.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.3.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.0.shortcut.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.0.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.0.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.0.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.1.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.1.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.1.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.2.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.2.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.2.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.3.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.3.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.3.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.3.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.3.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.3.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.4.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.4.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.4.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.4.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.4.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.4.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.5.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.5.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.5.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.5.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.5.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.5.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.6.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.6.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.6.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.6.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.6.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.6.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.7.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.7.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.7.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.7.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.7.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.7.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.8.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.8.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.8.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.8.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.8.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.8.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.9.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.9.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.9.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.9.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.9.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.9.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.10.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.10.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.10.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.10.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.10.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.10.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.11.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.11.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.11.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.11.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.11.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.11.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.12.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.12.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.12.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.12.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.12.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.12.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.13.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.13.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.13.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.13.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.13.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.13.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.14.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.14.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.14.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.14.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.14.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.14.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.15.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.15.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.15.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.15.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.15.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.15.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.16.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.16.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.16.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.16.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.16.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.16.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.17.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.17.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.17.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.17.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.17.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.17.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.18.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.18.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.18.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.18.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.18.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.18.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.19.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.19.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.19.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.19.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.19.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.19.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.20.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.20.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.20.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.20.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.20.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.20.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.21.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.21.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.21.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.21.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.21.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.21.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.22.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.22.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.22.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.22.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.22.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.22.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.0.shortcut.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.0.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.0.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.0.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.1.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.1.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.1.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.2.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.2.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.2.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.fc2.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n",
      "model inference has detected no elements of interest... so img will be skipped.\n"
     ]
    }
   ],
   "source": [
    "### Starting inference\n",
    "\n",
    "print (\"model inference started...\")\n",
    "cfg.MODEL.WEIGHTS = os.path.join(\"./output\", \"model_final.pth\")\n",
    "# cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.70\n",
    "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.50\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.25\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "img_path_list = os.listdir(img_in_dir)\n",
    "\n",
    "master_dict = []\n",
    "\n",
    "for img_path in img_path_list:\n",
    "\timg = cv2.imread(img_in_dir + img_path)\n",
    "\toutputs = predictor(img)\n",
    "\tif outputs[\"instances\"].__len__() > 0:\n",
    "\t\tprint(outputs)\n",
    "\t\tdata_dict = createDataDict(img_in_dir + img_path, outputs)\n",
    "\t\tvis = Visualizer(img[:, :, ::-1], scale=1)\n",
    "\t\tout = vis.draw_dataset_dict(data_dict)\n",
    "\t\tcv2.imwrite(\"./img_out/\"+img_path, out.get_image()[:, :, ::-1])\n",
    "\t\tmaster_dict.append(data_dict)\n",
    "\t\twith open(\"./results/data_dict.json\", \"w+\") as f:\n",
    "\t\t\tf.write(json.dumps(master_dict))\n",
    "\telse:\n",
    "\t\tprint(\"model inference has detected no elements of interest... so img will be skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtron2_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
